# borgesGPT
A tiny gptlike model with transformer architecture trained with the complete works of Jorge Luis Borges (&amp; other Authors, in spanish).  

This is a small language model made for eduactional purposes. My intention with this repo is to keep track of the improovements that I make while creating
my first language model. Including modifications of the dataset and the architecture. 

The basics of this model are taken from the fantastic video on building a GPT by Andrej Karpathy:
https://www.youtube.com/watch?v=kCc8FmEb1nY

### Results for a dataset containing aprox 8 million characters and character based encoding
Using a dataset containing aprox 13 million characters and a a character based encoding we achieved a validation loss (Crossentropy) of val_loss = 1.16, while the loss over the training data was train_loss = 1.12. Here is some text generated by the model
